{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Parameter Learning\n",
    "## Probabilistic Models UE\n",
    "### WS 2017\n",
    "\n",
    "---\n",
    "<div class=\"alert alert-warning\">\n",
    "**Due:** 21.11.2017, 11:00\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Please only submit the .ipynb file via Moodle!\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Hints:**  \n",
    "\n",
    "<ul>\n",
    "<li>You can use the `loadtxt` function to load data from text files, e.g.  \n",
    "    ```\n",
    "    data = loadtxt('train.txt', dtype=int)\n",
    "    ```  \n",
    "</li>\n",
    "<li>\n",
    "To count elements in a Numpy array, you can use the `bincount` function. Read the documentation for details, but in principle, it works like this:  \n",
    "\n",
    "```\n",
    ">>> a = array([0, 0, 1, 1, 0])\n",
    ">>> print bincount(a, minlength=2)\n",
    "[3 2]\n",
    "```  \n",
    "\n",
    "You can also just count elements in a loop, if you want.\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "After watching hours of medical dramas on television, a medicine freshman tries to figure out the perfect prediction model for heart diseases.\n",
    "Some of her computer science colleagues told her about how Bayesian networks can\n",
    "be used for symptom diagnosis, so she decides to model her ideas using this\n",
    "technique. In order to find out the (conditional) probability distributions, she needs to\n",
    "find a lot of training examples. One of her computer science colleagues\n",
    "cracks the computer system of the university clinic and copies a lot of medical\n",
    "patient data. The data is available as \"train.txt\" in Moodle. \n",
    "\n",
    "Throughout the whole assignment assume that all variables are boolean (false=0 or true=1). The data for this assignment is stored in two text files, \"train.txt\" and \"test.txt\". They contain 500 (train) and 5000 (test) \n",
    "samples of the following 5 random variables: \n",
    "\n",
    " - Column 0: S ... stress \n",
    " - Column 1: E ... easily catches cold  \n",
    " - Column 2: G ... genetic disposition \n",
    " - Column 3: I ... increased blood pressure\n",
    " - Column 4: H ... heart attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Finding a Model for Strokes\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Hint:** The next three exercises are quite similar. It might be a good idea to first write reusable functions for estimating certain kinds of probability distributions.\n",
    "</div>\n",
    "## 1.1 Model 1 (6 points)\n",
    "\n",
    "First, she decides to try out the following, very simple, model:\n",
    "\n",
    "<img style='width:100%;  max-width:400px;' src=\"bn_mod1.svg\">\n",
    "\n",
    " - **Which (conditional) probability tables are necessary to represent the Model 1?** Choose the finest factorisation possible - e.g., for a model $A \\rightarrow B \\rightarrow C$, you want to model $P(A)$, $P(B \\mid A)$, and $P(C \\mid B)$, and **not** just $P(A, B, C)$, or $P(A)$ and $P(B, C \\mid A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(G) P(S) P(E) P(I) P(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Estimate the probability tables (PDs) of Model 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from math import exp, expm1\n",
    "\n",
    "data = np.loadtxt('train.txt', dtype=int)\n",
    "data_test = np.loadtxt('test.txt', dtype=int)\n",
    "rows = data.shape[0]\n",
    "rows = 500.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "P_S = sum(data[:,0])/rows\n",
    "P_E = sum(data[:,1])/rows\n",
    "P_G = sum(data[:,2])/rows\n",
    "P_I = sum(data[:,3])/rows\n",
    "P_H = sum(data[:,4])/rows\n",
    "\n",
    "P_S = np.array([1-P_S, P_S])\n",
    "P_E = np.array([1-P_E, P_E])\n",
    "P_G = np.array([1-P_G, P_G])\n",
    "P_I = np.array([1-P_I, P_I])\n",
    "P_H = np.array([1-P_H, P_H])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Compute the log-likelihood of the train data given Model 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-421.6182524066589"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logL(P_S,P_E,P_G,P_I,P_H,data,rows):\n",
    "    cum = 0\n",
    "    for i in range(0,int(rows)):\n",
    "        cum = cum + math.log10(P_S[data[i,0]]*P_E[data[i,1]]*P_G[data[i,2]]*P_I[data[i,3]]*P_H[data[i,4]])\n",
    "    return cum\n",
    "logL(P_S,P_E,P_G,P_I,P_H,data,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model 2 (6 points)\n",
    "\n",
    "Not satisfied with the results, our freshman decides that Model 1 is probably too simple to represent the real world. She adds some edges to the model, and tries again:\n",
    "\n",
    "<img  style='width:100%;  max-width:400px;' src=\"bn_mod2.svg\">\n",
    "\n",
    " - **Which (conditional) probability tables are necessary to represent Model 2?** Choose the finest factorisation possible - e.g., for a model $A \\rightarrow B \\rightarrow C$, you want to model $P(A)$, $P(B \\mid A)$, and $P(C \\mid B)$, and **not** just $P(A, B, C)$, or $P(A)$ and $P(B, C \\mid A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(G), P(S), P(I|S,G), P(E|S),P(H|I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Estimate the probability tables (PDs) of Model 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P(G) and P(S) are priors therefore they will be the same as in 1.1\n",
    "               #GS\n",
    "count_Evid_of_I_00 = 0.0\n",
    "count_Evid_of_I_01 = 0.0\n",
    "count_Evid_of_I_10 = 0.0\n",
    "count_Evid_of_I_11 = 0.0\n",
    "\n",
    "count_I_given_00 = 0.0\n",
    "count_I_given_01 = 0.0\n",
    "count_I_given_10 = 0.0\n",
    "count_I_given_11 = 0.0\n",
    "\n",
    "count_H_given_0 = 0.0\n",
    "count_H_given_1 = 0.0\n",
    "\n",
    "count_E_given_0 = 0.0\n",
    "count_E_given_1 = 0.0\n",
    "#compute I\n",
    "for i in range(0,int(rows)):\n",
    "    #computing I #00\n",
    "    if data[i,2] == 0 and data[i,0] == 0:\n",
    "        count_Evid_of_I_00 = count_Evid_of_I_00+1\n",
    "        if data[i,3] == 1:\n",
    "            count_I_given_00 = count_I_given_00 + 1\n",
    "    #11\n",
    "    if data[i,2] == 1 and data[i,0] == 1:\n",
    "        count_Evid_of_I_11 = count_Evid_of_I_11+1\n",
    "        if data[i,3] == 1:\n",
    "            count_I_given_11 = count_I_given_11 + 1\n",
    "     #10       \n",
    "    if data[i,2] == 1 and data[i,0] == 0:\n",
    "        count_Evid_of_I_10 = count_Evid_of_I_10+1\n",
    "        if data[i,3] == 1:\n",
    "            count_I_given_10 = count_I_given_10 + 1\n",
    "    #01\n",
    "    if data[i,2] == 0 and data[i,0] == 1:\n",
    "        count_Evid_of_I_01 = count_Evid_of_I_01+1\n",
    "        if data[i,3] == 1:\n",
    "            count_I_given_01 = count_I_given_01 + 1\n",
    "    #compute H\n",
    "    if data[i,3] == 0 and data[i,4] == 1:\n",
    "        count_H_given_0 = count_H_given_0+1\n",
    "    \n",
    "    if data[i,3] == 1 and data[i,4] == 1:\n",
    "        count_H_given_1 = count_H_given_1+1\n",
    "        \n",
    "    #compute E\n",
    "    if data[i,0] == 0 and data[i,1] == 1:\n",
    "        count_E_given_0 = count_E_given_0+1\n",
    "    \n",
    "    if data[i,0] == 1 and data[i,1] == 1:\n",
    "        count_E_given_1 = count_E_given_1+1\n",
    "        \n",
    "\n",
    "P_I_bayes_m1 = np.array([\n",
    "               [1-(count_I_given_00/count_Evid_of_I_00),(count_I_given_00/count_Evid_of_I_00)], #G0S0\n",
    "               [1-(count_I_given_10/count_Evid_of_I_10),(count_I_given_10/count_Evid_of_I_10)],  #G1S0\n",
    "               [1-(count_I_given_01/count_Evid_of_I_01),(count_I_given_01/count_Evid_of_I_01)],  #G0S1\n",
    "               [1-(count_I_given_11/count_Evid_of_I_11),(count_I_given_11/count_Evid_of_I_11)]]) #G1S1\n",
    "\n",
    "P_H_bayes_m1 = np.array([\n",
    "                     [1-(count_H_given_0/((P_I[0])*rows)),(count_H_given_0/((P_I[0])*rows))], #I0\n",
    "                     [1-(count_H_given_1/(P_I[1]*rows)),(count_H_given_1/(P_I[1]*rows))]          #I1\n",
    "                    ]) \n",
    "\n",
    "P_E_bayes_m1 = np.array([\n",
    "                     [1-(count_E_given_0/((P_S[0])*rows)),(count_E_given_0/((P_S[0])*rows))], #S0\n",
    "                     [1-(count_E_given_1/(P_S[1]*rows)),(count_E_given_1/(P_S[1]*rows))]      #S1\n",
    "                    ]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Compute the log-likelihood of the train data given Model 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-408.56817038870247"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logL1(P_S,P_E,P_G,P_I,P_H,data,rows):\n",
    "    cum = 0\n",
    "    for i in range(0,int(rows)):\n",
    "        if data[i,0] == 0 and data[i,2] == 0:\n",
    "            access = 0\n",
    "        elif data[i,0] == 0 and data[i,2] == 1:\n",
    "            access = 1\n",
    "        elif data[i,0] == 1 and data[i,2] == 0:\n",
    "            access = 2\n",
    "        elif data[i,0] == 1 and data[i,2] == 1:\n",
    "            access = 3\n",
    "        cum = cum + math.log10(P_S[data[i,0]]*P_E[data[i,0]][data[i,1]]*P_G[data[i,2]]*P_I[access][data[i,3]]*P_H[data[i,3]][data[i,4]])                                     \n",
    "    return cum\n",
    "logL1(P_S,P_E_bayes_m1,P_G,P_I_bayes_m1,P_H_bayes_m1,data,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model 3 (6 points)\n",
    "\n",
    "Finally, she decides to try out an even more complex model:\n",
    "\n",
    "<img  style='width:100%;  max-width:400px;' src=\"bn_mod3.svg\">\n",
    "\n",
    " - **Which (conditional) probability tables are necessary to represent Model 3?** Choose the finest factorisation possible - e.g., for a model $A \\rightarrow B \\rightarrow C$, you want to model $P(A)$, $P(B \\mid A)$, and $P(C \\mid B)$, and **not** just $P(A, B, C)$, or $P(A)$ and $P(B, C \\mid A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(G),p(S|G),P(I|G,S),P(E|S,I),P(H|E,I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Estimate the probability tables (PDs) of Model 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#P(G) is still the same, however p(S) changed as its dependent on G in the upper mentioned model\n",
    "count_S_given_0 = 0.0\n",
    "count_S_given_1 = 0.0\n",
    "\n",
    "count_Evid_of_I_00 = 0.0\n",
    "count_Evid_of_I_10 = 0.0\n",
    "count_Evid_of_I_01 = 0.0\n",
    "count_Evid_of_I_11 = 0.0\n",
    "\n",
    "count_I_given_00 = 0.0\n",
    "count_I_given_10 = 0.0\n",
    "count_I_given_01 = 0.0\n",
    "count_I_given_11 = 0.0\n",
    "\n",
    "count_Evid_of_E_00 = 0.0\n",
    "count_Evid_of_E_10 = 0.0\n",
    "count_Evid_of_E_01 = 0.0\n",
    "count_Evid_of_E_11 = 0.0\n",
    "\n",
    "count_E_given_00 = 0.0\n",
    "count_E_given_10 = 0.0\n",
    "count_E_given_01 = 0.0\n",
    "count_E_given_11 = 0.0\n",
    "\n",
    "count_Evid_of_H_00 = 0.0\n",
    "count_Evid_of_H_10 = 0.0\n",
    "count_Evid_of_H_01 = 0.0\n",
    "count_Evid_of_H_11 = 0.0\n",
    "\n",
    "count_H_given_00 = 0.0\n",
    "count_H_given_10 = 0.0\n",
    "count_H_given_01 = 0.0\n",
    "count_H_given_11 = 0.0\n",
    "\n",
    "count_S_given_0 = 0.0\n",
    "count_S_given_1 = 0.0\n",
    "\n",
    "for i in range(0,int(rows)):\n",
    "   \n",
    "        \n",
    "    #compute I\n",
    "    #computing I #00\n",
    "    if data[i,2] == 0 and data[i,0] == 0:\n",
    "        count_Evid_of_I_00 = count_Evid_of_I_00+1\n",
    "        if data[i,3] == 1:\n",
    "            count_I_given_00 = count_I_given_00 + 1\n",
    "    #11\n",
    "    if data[i,2] == 1 and data[i,0] == 1:\n",
    "        count_Evid_of_I_11 = count_Evid_of_I_11+1\n",
    "        if data[i,3] == 1:\n",
    "            count_I_given_11 = count_I_given_11 + 1\n",
    "     #10       \n",
    "    if data[i,2] == 1 and data[i,0] == 0:\n",
    "        count_Evid_of_I_10 = count_Evid_of_I_10+1\n",
    "        if data[i,3] == 1:\n",
    "            count_I_given_10 = count_I_given_10 + 1\n",
    "    #01\n",
    "    if data[i,2] == 0 and data[i,0] == 1:\n",
    "        count_Evid_of_I_01 = count_Evid_of_I_01+1\n",
    "        if data[i,3] == 1:\n",
    "            count_I_given_01 = count_I_given_01 + 1\n",
    "            \n",
    "    #compute E\n",
    "    #computing E #00\n",
    "    if data[i,0] == 0 and data[i,3] == 0:\n",
    "        count_Evid_of_E_00 = count_Evid_of_E_00+1\n",
    "        if data[i,1] == 1:\n",
    "            count_E_given_00 = count_E_given_00 + 1\n",
    "    #11\n",
    "    if data[i,0] == 1 and data[i,3] == 1:\n",
    "        count_Evid_of_E_11 = count_Evid_of_E_11+1\n",
    "        if data[i,1] == 1:\n",
    "            count_E_given_11 = count_E_given_11 + 1\n",
    "     #10       \n",
    "    if data[i,0] == 1 and data[i,3] == 0:\n",
    "        count_Evid_of_E_10 = count_Evid_of_E_10+1\n",
    "        if data[i,1] == 1:\n",
    "            count_E_given_10 = count_E_given_10 + 1\n",
    "    #01\n",
    "    if data[i,0] == 0 and data[i,3] == 1:\n",
    "        count_Evid_of_E_01 = count_Evid_of_E_01+1\n",
    "        if data[i,1] == 1:\n",
    "            count_E_given_01 = count_E_given_01 + 1\n",
    "    \n",
    "    \n",
    "    #compute H\n",
    "    #computing H #00\n",
    "    if data[i,1] == 0 and data[i,3] == 0:\n",
    "        count_Evid_of_H_00 = count_Evid_of_H_00+1\n",
    "        if data[i,4] == 1:\n",
    "            count_H_given_00 = count_H_given_00 + 1\n",
    "    #11\n",
    "    if data[i,1] == 1 and data[i,3] == 1:\n",
    "        count_Evid_of_H_11 = count_Evid_of_H_11+1\n",
    "        if data[i,4] == 1:\n",
    "            count_H_given_11 = count_H_given_11 + 1\n",
    "     #10       \n",
    "    if data[i,1] == 1 and data[i,3] == 0:\n",
    "        count_Evid_of_H_10 = count_Evid_of_H_10+1\n",
    "        if data[i,4] == 1:\n",
    "            count_H_given_10 = count_H_given_10 + 1\n",
    "    #01\n",
    "    if data[i,1] == 0 and data[i,3] == 1:\n",
    "        count_Evid_of_H_01 = count_Evid_of_H_01+1\n",
    "        if data[i,4] == 1:\n",
    "            count_H_given_01 = count_H_given_01 + 1\n",
    "    \n",
    "    #compute S\n",
    "    if data[i,2] == 0 and data[i,0] == 1:\n",
    "        count_S_given_0 = count_S_given_0+1\n",
    "    \n",
    "    if data[i,2] == 1 and data[i,0] == 1:\n",
    "        count_S_given_1 = count_S_given_1+1\n",
    "            \n",
    "P_I_bayes_m2 = np.array([\n",
    "               [1-(count_I_given_00/count_Evid_of_I_00),(count_I_given_00/count_Evid_of_I_00)], #G0S0\n",
    "               [1-(count_I_given_10/count_Evid_of_I_10),(count_I_given_10/count_Evid_of_I_10)],  #G1S0\n",
    "               [1-(count_I_given_01/count_Evid_of_I_01),(count_I_given_01/count_Evid_of_I_01)],  #G0S1\n",
    "               [1-(count_I_given_11/count_Evid_of_I_11),(count_I_given_11/count_Evid_of_I_11)]]) #G1S1\n",
    "P_E_bayes_m2 = np.array([\n",
    "               [1-(count_E_given_00/count_Evid_of_E_00),(count_E_given_00/count_Evid_of_E_00)], #S0I0\n",
    "               [1-(count_E_given_10/count_Evid_of_E_10),(count_E_given_10/count_Evid_of_E_10)], #S1I0\n",
    "               [1-(count_E_given_01/count_Evid_of_E_01),(count_E_given_01/count_Evid_of_E_01)],  #S0I1\n",
    "               [1-(count_E_given_11/count_Evid_of_E_11),(count_E_given_11/count_Evid_of_E_11)]]) #S1I1\n",
    "P_H_bayes_m2 = np.array([\n",
    "               [1-(count_H_given_00/count_Evid_of_H_00),(count_H_given_00/count_Evid_of_H_00)], #E0I0\n",
    "               [1-(count_H_given_10/count_Evid_of_H_10),(count_H_given_10/count_Evid_of_H_10)],  #E1I0\n",
    "               [1-(count_H_given_01/count_Evid_of_H_01),(count_H_given_01/count_Evid_of_H_01)],  #E0I1\n",
    "               [1-(count_H_given_11/count_Evid_of_H_11),(count_H_given_11/count_Evid_of_H_11)]]) #E1I1\n",
    "\n",
    "P_S_bayes_m2 = np.array([\n",
    "                     [1-(count_S_given_0/((P_G[0])*rows)),(count_S_given_0/((P_G[0])*rows))], #G0\n",
    "                     [1-(count_S_given_1/(P_G[1]*rows)),(count_S_given_1/(P_G[1]*rows))]          #G1\n",
    "                    ]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Compute the log-likelihood of the train data given Model 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-408.12130459532096"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logL2(P_S,P_E,P_G,P_I,P_H,data,rows):\n",
    "    cum = 0\n",
    "    for i in range(0,int(rows)):\n",
    "        if data[i,0] == 0 and data[i,2] == 0:\n",
    "            access = 0\n",
    "        elif data[i,0] == 0 and data[i,2] == 1:\n",
    "            access = 1\n",
    "        elif data[i,0] == 1 and data[i,2] == 0:\n",
    "            access = 2\n",
    "        elif data[i,0] == 1 and data[i,2] == 1:\n",
    "            access = 3\n",
    "            \n",
    "        if data[i,0] == 0 and data[i,3] == 0:\n",
    "            access2 = 0\n",
    "        elif data[i,0] == 1 and data[i,3] == 0:\n",
    "            access2 = 1\n",
    "        elif data[i,0] == 0 and data[i,3] == 1:\n",
    "            access2 = 2\n",
    "        elif data[i,0] == 1 and data[i,3] == 1:\n",
    "            access2 = 3\n",
    "            \n",
    "        if data[i,1] == 0 and data[i,3] == 0:\n",
    "            access3 = 0\n",
    "        elif data[i,1] == 1 and data[i,3] == 0:\n",
    "            access3 = 1\n",
    "        elif data[i,1] == 0 and data[i,3] == 1:\n",
    "            access3 = 2\n",
    "        elif data[i,1] == 1 and data[i,3] == 1:\n",
    "            access3 = 3\n",
    "        cum = cum + math.log10((P_S[data[i,2]][data[i,0]])*P_E[access2][data[i,1]]*P_G[data[i,2]]*P_I[access][data[i,3]]*P_H[access3][data[i,4]])                                     \n",
    "    return cum\n",
    "logL2(P_S_bayes_m2,P_E_bayes_m2,P_G,P_I_bayes_m2,P_H_bayes_m2,data,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Comparison\n",
    "\n",
    "## 2.1 Compare Train Log-Likelihoods (3 points)\n",
    "\n",
    "Compare the log-likelihoods of the training data given the three models. Why are they different? What does it **mean** that the data has a higher log-likelihood for a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model 1 Log-likelihood:-421.618252407\n",
    "Model 2 Log-likelihood:-408.568170389\n",
    "Model 3 Log-likelihood:-408.121304595\n",
    "\n",
    "These results show that the training data is most likely to be generated from model 3 than model 2 followed by model 1.\n",
    "\n",
    "They are different due to the architecture of the bayesian net that corresponds to each model. Model 3 adds more dependencies as a way to fit more precisely the given training data whereas model 1 assumes no dependancies-underfittign- and therefore, it has a very low likelihood compared to the other 2 models.\n",
    "\n",
    "The data has a higher likelihood for a model means that this model -in this case, model 3- has a higher probability of generating these data -training data-."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compare Test Log-Likelihoods (3 points)\n",
    "\n",
    "The computer science colleague manages to obtain more data from the clinic's network and stores it in the file `test.txt`. Our medicine freshman is eager to try the models on the new data.\n",
    "\n",
    "Compute the log-likelihood of the test data for each of the three models. **Do not learn probability tables!** Use the probability tables you computed in 1).\n",
    "\n",
    "**What is the difference compared to the log-likelihoods of the training dataset** (ignore the magnitude difference due to different number of data points in each set)? Explain the difference! From your results, which of the models would you select, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4145.54025994\n",
      "-4002.84858896\n",
      "-4007.19714656\n"
     ]
    }
   ],
   "source": [
    "print(logL(P_S,P_E,P_G,P_I,P_H,data_test,5000))\n",
    "print(logL1(P_S,P_E_bayes_m1,P_G,P_I_bayes_m1,P_H_bayes_m1,data_test,5000.0))\n",
    "print(logL2(P_S_bayes_m2,P_E_bayes_m2,P_G,P_I_bayes_m2,P_H_bayes_m2,data_test,5000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model 1 Log-likelihood:-4145.54025994\n",
    "Model 2 Log-likelihood:-4002.84858896\n",
    "Model 3 Log-likelihood:-4007.19714656\n",
    "Its clearly shown that model 3 overfitts that training data and therefore it failed to generalize on the testing data as compared to model 2 that shows a better likelihood. ofcourse model 1 has a lower value than the rest due to the fact that it underfitts.\n",
    "\n",
    "Model 2 would be selected as it has the higher likelihood on the test data which means that it was more capable of generalizing on the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
